{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De benodigde packages\n",
    "import getpass         # Package om een paswoordveldje te genereren.\n",
    "import mysql.connector # MySQL package\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from os.path import dirname\n",
    "\n",
    "# Imports van binnen onze eigen repository (verbetertools)\n",
    "this_dir = os.getcwd()\n",
    "root_dir = dirname(this_dir)\n",
    "docs_dir = os.path.join(root_dir, 'docs')\n",
    "source_dir = os.path.join(root_dir, 'src')\n",
    "script_dir = os.path.join(root_dir, 'scripts')\n",
    "solution_dir = os.path.join(root_dir, 'solution')\n",
    "sys.path.append(source_dir)\n",
    "\n",
    "import db_project\n",
    "from db_project import (\n",
    "    verbind_met_GB,\n",
    "    run_query,\n",
    "    res_to_df,\n",
    "    run_external_script,\n",
    "    evaluate_script,\n",
    "    parse_markdown\n",
    ") \n",
    "\n",
    "verification_file = os.path.join(docs_dir, '05-verification.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1. Inleiding\n",
       "\n",
       "In deze notebook kan je je oplossingen voor de eerste 3 queries testen. \n",
       "\n",
       "- Eerst wordt je script automatisch gerund met verschillende parameters\n",
       "- De resultaten worden opgeslagen in `.csv` files (in de `out` folder)\n",
       "- Die `.csv` files worden vergeleken met de `.csv` files van de oplossing (te vinden in de `solution` folder).\n",
       "- Elke query krijgt een score toegekend. Cf. https://en.wikipedia.org/wiki/F1_score.  \n",
       "- Een kort rapport wordt weergegeven die je pointers kan geven over wat er mis is met je query. \n",
       "    - TP: True Positives\n",
       "    - TN: True Negatives\n",
       "    - FP: False Positives\n",
       "    \n",
       "Na release van de modeloplossingen kan je natuurlijk alles controleren.\n",
       "    \n",
       "## 2. Uitvoeren\n",
       "\n",
       "Dit proces bestaat uit 3 stappen.\n",
       "\n",
       "1. Eerst dien je terug verbinding te maken met de gegevensbank.\n",
       "2. Dan dien je de filename van je ingevulde script in te vullen. Je \"ingevulde script\" is het bestand dat **ALLE INGEVULDE FUNCTIES EN NIETS ANDERS** bevat. Hieronder importeren we het script `example_solution.py` van in de `scripts` folder.\n",
       "3. Run het script. Dit doen we met behulp van de `run_external_script` functie die we eerder importeerden. (De parameters en de kolomnamen worden automatisch ingelezen uit de `.json` files in de `solution` folder.)\n",
       "    \n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(parse_markdown(verification_file, section_number=[1,2]))) # Lees tekst in uit docs folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAP 01: Verbindingsobject aanmaken\n",
    "\n",
    "username = 'root'      # Vervang dit als je via een andere user queries stuurt\n",
    "hostname = 'localhost' # Als je een databank lokaal draait, is dit localhost.\n",
    "db = 'db-project'          # Naam van de gegevensbank op je XAMPP Mysql server\n",
    "\n",
    "# We verbinden met de gegevensbank\n",
    "c = verbind_met_GB(username, hostname, db, password='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAP 02: Specifieer filename (vul jouw filename hier dus in!)\n",
    "\n",
    "filename = os.path.join(script_dir, 'demo_solution_DELETEME.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAP 03: Run je oplossingen\n",
    "\n",
    "params_filename = \"all_q_params.json\"\n",
    "\n",
    "run_external_script(filename, c, params_fname=params_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 3. Evaluatie\n",
       "\n",
       "Het externe script is nu uitgevoerd met de door ons vastgelegde parameters. Op die manier zijn er `.csv` files gemaakt in de `out` directory.\n",
       "\n",
       "Die `.csv` files worden nu vergeleken met door ons aangemaakte `.csv` files van de correcte oplossingen. Op die manier wordt de score berekend. In eerste instantie kan je enkel de eerste 3 queries controleren. De rest kan pas gecontroleerd worden nadat de modeloplossingen allemaal vrijgegeven zijn.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(parse_markdown(verification_file, section_number=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating query 1\n",
      "Evaluating query 2\n",
      "Evaluating query 3\n",
      "Evaluating query 4\n",
      "Evaluating query 5\n",
      "Evaluating query 6\n",
      "Evaluating query 7\n",
      "Evaluating query 8\n",
      "Evaluating query 9\n",
      "Evaluating query 10\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_script(filename,  params_fname=params_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 4. Reports\n",
       "\n",
       "Om te kijken wat je score was, cf. de `out` folder:\n",
       "   - in `out/reports` vind je gegeneerde reports. \n",
       "       - Het `execution_report` is een log van het uitvoeren van je script.\n",
       "       - Het `evaluation_report` is een kleine samenvatting van je behaalde scores.\n",
       "   - in `out/results` vind je de `.csv` files die je queries geproduceerd hebben.\n",
       "\n",
       "\n",
       "        \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(parse_markdown(verification_file, section_number=4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db-2020",
   "language": "python",
   "name": "db-2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
